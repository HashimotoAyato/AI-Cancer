{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de65812a",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f22fa81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "from torchmetrics.functional import precision_recall, accuracy, f1_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a430e546",
   "metadata": {},
   "source": [
    "### Data Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa7c146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning data directory\n",
    "DATASET_NAME = 'dataset20220823'\n",
    "DATA_ROOT = '/export/hashimoto/DATA/' + DATASET_NAME + '/'\n",
    "JSON_FILE = DATA_ROOT + 'dataset.json'\n",
    "IMAGE_DIR = DATA_ROOT + 'img/'\n",
    "SLICE_NUM = 3\n",
    "IMAGE_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5445e7c",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42d6ab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_SIZE = 40/176\n",
    "GAUSSIAN_SIGMA = 5\n",
    "\n",
    "WINDOW_SIZE = 15\n",
    "WINDOW_STRIDE = 3\n",
    "WINDOW_SIGMA = 10\n",
    "\n",
    "VMIN = 240\n",
    "VMAX = 255\n",
    "\n",
    "PROJECT = 'A6c'\n",
    "LOAD_DIR = '/export/hashimoto/Results/' + PROJECT + '/'\n",
    "LOAD_FILE = '_model_best.pth'\n",
    "SAVE_DIR = '/export/hashimoto/ProbabilityMap/' + PROJECT + '/'\n",
    "\n",
    "# Select using GPU\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a72659",
   "metadata": {},
   "source": [
    "### Directry and Log files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd989ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project name (using for save and log directory name)\n",
    "FOLDER = 'A6c'\n",
    "SAVE_DIR = '/export/hashimoto/ProbabilityMap/' + FOLDER + '/'\n",
    "os.makedirs(SAVE_DIR, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01472c6",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "146f342d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid data:1600 (last element:49_1_9)\n",
      "train data:5440 (first element:50_1_1)\n"
     ]
    }
   ],
   "source": [
    "with open(JSON_FILE, 'r') as f:\n",
    "    jsondata = json.load(f)\n",
    "\n",
    "files, labels = [], []\n",
    "for key in jsondata.keys():\n",
    "    info = key\n",
    "    t2, adc, t1d = jsondata[key]['T2'], jsondata[key]['ADC'], jsondata[key]['T1D']\n",
    "    \n",
    "    files.append({'info':info, 't2':t2, 'adc':adc, 't1d':t1d})\n",
    "    labels.append(jsondata[key]['ROI'])\n",
    "\n",
    "# splid data (mtabun mazatte simau)\n",
    "valid_files, valid_labels = files[:int(VALID_SIZE*len(files))], labels[:int(VALID_SIZE*len(files))]\n",
    "train_files, train_labels = files[int(VALID_SIZE*len(files)):], labels[int(VALID_SIZE*len(files)):]\n",
    "del files, labels\n",
    "\n",
    "# print data list\n",
    "print('valid data:{} (last element:{})'.format(len(valid_files), valid_files[-1]['info']))\n",
    "\n",
    "print('train data:{} (first element:{})'.format(len(train_files), train_files[0]['info']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9fbf18",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f9d7f82",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# int:0-255 [Numpy] => float:0.0-1.0 [Tensor]\n",
    "class PixelToTensor(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, x):\n",
    "        data = x / 255.0\n",
    "        return torch.tensor(data, dtype = torch.float)\n",
    "    \n",
    "# fullImages (int:0-255 [Numpy]), windowCenter(int [array]) => maskImages (float:0.0-1.0 [Tensor])\n",
    "class PositionToCropImages(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, fullImages, center):\n",
    "        # 0 padding\n",
    "        frame = torch.zeros(len(fullImages), len(fullImages[0]), IMAGE_SIZE+WINDOW_SIZE, IMAGE_SIZE+WINDOW_SIZE)\n",
    "        frame[:,:,int(WINDOW_SIZE/2):int(WINDOW_SIZE/2)+IMAGE_SIZE,int(WINDOW_SIZE/2):int(WINDOW_SIZE/2)+IMAGE_SIZE] = fullImages\n",
    "        # offset\n",
    "        offsetCenter = center + int(WINDOW_SIZE/2)\n",
    "    \n",
    "        # crop range\n",
    "        rangex = np.array([offsetCenter[0]-int(WINDOW_SIZE/2), offsetCenter[0]+int(WINDOW_SIZE/2)+1]).astype(int)\n",
    "        rangey = np.array([offsetCenter[1]-int(WINDOW_SIZE/2), offsetCenter[1]+int(WINDOW_SIZE/2)+1]).astype(int)\n",
    "\n",
    "        \n",
    "        cropImages = frame[:, :, rangex[0]:rangex[1], rangey[0]:rangey[1]]\n",
    "\n",
    "        return cropImages\n",
    "\n",
    "class Dataset_CropImage(torch.utils.data.Dataset):\n",
    "    def __init__(self, files, labels):\n",
    "        self.files = files\n",
    "        self.labels = labels\n",
    "        self.pixelToTensor = PixelToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # info\n",
    "        info = self.files[idx]['info']\n",
    "\n",
    "        # exception\n",
    "        if len(self.files[idx]['t1d']) != 20:\n",
    "            print('info:' + info)\n",
    "            print('t1dlen:' + str(len(self.files[idx]['t1d'])))\n",
    "\n",
    "        # fullImages[volume][slice][x][y] voluem0:T2WI, volume1:ADC Map, volume2:T1WI, volume3:CE-T1WI\n",
    "        fullImages = np.zeros((22, SLICE_NUM, IMAGE_SIZE, IMAGE_SIZE))\n",
    "        for i in range(SLICE_NUM):\n",
    "            fullImages[0][i] = np.array(Image.open(IMAGE_DIR + self.files[idx]['t2'][i]))\n",
    "            fullImages[1][i] = np.array(Image.open(IMAGE_DIR + self.files[idx]['adc'][i])) \n",
    "            for j in range(20):     \n",
    "                fullImages[2+j][i] = np.array(Image.open(IMAGE_DIR + self.files[idx]['t1d'][j][i]))\n",
    "        fullImages = self.pixelToTensor(fullImages)\n",
    "\n",
    "        output_size = int((IMAGE_SIZE + 2 * int(WINDOW_SIZE/2) - (WINDOW_SIZE-1) - 1) / WINDOW_STRIDE + 1)\n",
    "        cropImages = torch.zeros(output_size, output_size, len(fullImages), len(fullImages[0]), WINDOW_SIZE, WINDOW_SIZE)\n",
    "        \n",
    "        # zero padding\n",
    "        frame = torch.zeros(len(fullImages), len(fullImages[0]), IMAGE_SIZE+WINDOW_SIZE-1, IMAGE_SIZE+WINDOW_SIZE-1)\n",
    "        frame[:,:,int(WINDOW_SIZE/2):int(WINDOW_SIZE/2)+IMAGE_SIZE,int(WINDOW_SIZE/2):int(WINDOW_SIZE/2)+IMAGE_SIZE] = fullImages\n",
    "\n",
    "        for i in range(output_size):\n",
    "            for j in range(output_size):\n",
    "                cropImages[i,j] = frame[:,:,i*WINDOW_STRIDE:i*WINDOW_STRIDE+WINDOW_SIZE, j*WINDOW_STRIDE:j*WINDOW_STRIDE+WINDOW_SIZE]\n",
    "        \n",
    "        return info, cropImages\n",
    "\n",
    "class Dataset_FullImage(torch.utils.data.Dataset):\n",
    "    def __init__(self, files, labels):\n",
    "        self.files = files\n",
    "        self.labels = labels\n",
    "        self.pixelToTensor = PixelToTensor()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # info\n",
    "        info = self.files[idx]['info']\n",
    "        \n",
    "        # exception\n",
    "        if len(self.files[idx]['t1d']) != 20:\n",
    "            print('info:' + info)\n",
    "            print('t1dlen:' + str(len(self.files[idx]['t1d'])))\n",
    "        \n",
    "        # fullImages[volume][slice][x][y] voluem0:T2WI, volume1:ADC Map, volume2:T1WI, volume3:CE-T1WI\n",
    "        fullImages = np.zeros((22, SLICE_NUM, IMAGE_SIZE, IMAGE_SIZE))\n",
    "        for i in range(SLICE_NUM):\n",
    "            fullImages[0][i] = np.array(Image.open(IMAGE_DIR + self.files[idx]['t2'][i]))\n",
    "            fullImages[1][i] = np.array(Image.open(IMAGE_DIR + self.files[idx]['adc'][i])) \n",
    "            for j in range(20):     \n",
    "                fullImages[2+j][i] = np.array(Image.open(IMAGE_DIR + self.files[idx]['t1d'][j][i]))\n",
    "        fullImages = self.pixelToTensor(fullImages)\n",
    "\n",
    "        # convert MASK_IMAGE to ROI position(index)\n",
    "        maskImage = np.array(Image.open(IMAGE_DIR + self.labels[idx]))\n",
    "        maskImage = maskImage[:,:,1]\n",
    "        roiCenter = np.array([0,0], int)\n",
    "        for i in range(len(maskImage)):\n",
    "            for j in range(len(maskImage[0])):\n",
    "                if maskImage[roiCenter[0],roiCenter[1]] < maskImage[i,j]:\n",
    "                    roiCenter = np.array([i, j], int)\n",
    "\n",
    "        # generate gaussian roi map\n",
    "        gaussianRoi = np.zeros((IMAGE_SIZE, IMAGE_SIZE))\n",
    "        for i in range(IMAGE_SIZE):\n",
    "            for j in range(IMAGE_SIZE):\n",
    "                gaussianRoi[i, j] =  np.exp( -((roiCenter[0]-i)**2 + (roiCenter[1]-j)**2) / (2*((GAUSSIAN_SIGMA+1)**2)) )\n",
    "        gaussianRoi = self.pixelToTensor(gaussianRoi*255.0)  \n",
    "        \n",
    "        return info, fullImages, gaussianRoi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15d70b5",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90161ec2",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class InputEncoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InputEncoder, self).__init__()\n",
    "        self.input = 0\n",
    "        self.layer1 = 0\n",
    "        self.output = 0\n",
    "        \n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 3, padding = 1),\n",
    "            torch.nn.BatchNorm2d(32), torch.nn.ReLU())\n",
    "        \n",
    "        self.conv2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, padding = 1),\n",
    "            torch.nn.BatchNorm2d(32), torch.nn.ReLU())\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.layer1 = self.conv1(self.input)\n",
    "        self.output = self.conv2(self.layer1)\n",
    "        return self.output \n",
    "\n",
    "class DynamicEncoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DynamicEncoder, self).__init__()\n",
    "        self.input = self.layer1 = self.output = 0\n",
    "        \n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(in_channels = 1, out_channels = 32, kernel_size = (7,3,3), padding = (3,1,1), stride = (3,1,1)),\n",
    "            torch.nn.BatchNorm3d(32), torch.nn.ReLU())\n",
    "        \n",
    "        self.conv2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(in_channels = 32, out_channels = 32, kernel_size = (7,3,3), padding = (0,1,1), stride = 1),\n",
    "            torch.nn.BatchNorm3d(32), torch.nn.ReLU())\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.layer1 = self.conv1(input)\n",
    "        self.output = self.conv2(self.layer1)\n",
    "        self.output = torch.squeeze(self.output)\n",
    "        return self.output    \n",
    "\n",
    "class GlobalEncoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalEncoder, self).__init__()\n",
    "        self.input = 0\n",
    "        # Encoder layer\n",
    "        self.layer1 = self.layer2 = self.layer3 = 0\n",
    "        # Decoder layer\n",
    "        self.layer4 = self.layer5 = self.layer6 = 0\n",
    "        self.positionToCropImages = PositionToCropImages()\n",
    "        # output\n",
    "        self.cropFeature = self.gaussian = 0\n",
    "        \n",
    "        self.step1 = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(kernel_size = 2, stride = 2), torch.nn.Dropout(),\n",
    "            torch.nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, padding = 1),\n",
    "            torch.nn.BatchNorm2d(256), torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1),\n",
    "            torch.nn.BatchNorm2d(256), torch.nn.ReLU())\n",
    "\n",
    "        self.step2 = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(kernel_size = 2, stride = 2), torch.nn.Dropout(),\n",
    "            torch.nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 3, padding = 1),\n",
    "            torch.nn.BatchNorm2d(512), torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1),\n",
    "            torch.nn.BatchNorm2d(512), torch.nn.ReLU())\n",
    "        \n",
    "        self.step3 = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(kernel_size = 2, stride = 2), torch.nn.Dropout(),\n",
    "            torch.nn.Conv2d(in_channels = 512, out_channels = 1024, kernel_size = 3, padding = 1),\n",
    "            torch.nn.BatchNorm2d(1024), torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels = 1024, out_channels =1024, kernel_size = 3, padding = 1),\n",
    "            torch.nn.BatchNorm2d(1024), torch.nn.ReLU())\n",
    "        \n",
    "        self.deconv1 = torch.nn.ConvTranspose2d(in_channels = 1024, out_channels = 512, kernel_size = 2, stride = 2)\n",
    "        \n",
    "        self.step4 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels = 1024, out_channels = 512, kernel_size = 3, padding = 1),\n",
    "            torch.nn.BatchNorm2d(512), torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1),\n",
    "            torch.nn.BatchNorm2d(512), torch.nn.ReLU(), torch.nn.Dropout())\n",
    "        \n",
    "        self.deconv2 = torch.nn.ConvTranspose2d(in_channels = 512, out_channels = 256, kernel_size = 2, stride = 2)\n",
    "        \n",
    "        self.step5 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels = 512, out_channels = 256, kernel_size = 3, padding = 1),\n",
    "            torch.nn.BatchNorm2d(256), torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1),\n",
    "            torch.nn.BatchNorm2d(256), torch.nn.ReLU(), torch.nn.Dropout())\n",
    "        \n",
    "        self.deconv3 = torch.nn.ConvTranspose2d(in_channels = 256, out_channels = 128, kernel_size = 2, stride = 2)\n",
    "        \n",
    "        self.step6 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels =128+128, out_channels = 128, kernel_size = 3, padding = 1),\n",
    "            torch.nn.BatchNorm2d(128), torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, padding = 1),\n",
    "            torch.nn.BatchNorm2d(128), torch.nn.ReLU(), torch.nn.Dropout())\n",
    "        \n",
    "        self.step7 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels = 128, out_channels = 2, kernel_size = 1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Softmax(dim = 1))  \n",
    "        \n",
    "    def forward(self, fullImages, cropCenter):\n",
    "        self.input = fullImages\n",
    "        # Encode\n",
    "        self.layer1 = self.step1(self.input)\n",
    "        self.layer2 = self.step2(self.layer1)\n",
    "        self.layer3 = self.step3(self.layer2)\n",
    "        \n",
    "        # Decode\n",
    "        self.layer4 = self.step4(torch.cat([self.deconv1(self.layer3), self.layer2], dim = 1))\n",
    "        self.layer5 = self.step5(torch.cat([self.deconv2(self.layer4), self.layer1], dim = 1))\n",
    "        self.layer6 = self.step6(torch.cat([self.deconv3(self.layer5), self.input], dim = 1))\n",
    "\n",
    "        # output\n",
    "        self.cropFeature = torch.zeros(len(self.layer6), len(self.layer6[0]), WINDOW_SIZE, WINDOW_SIZE)\n",
    "        for i in range(len(self.layer6)):\n",
    "            self.cropFeature[i] = self.positionToCropImages(self.layer6[i].unsqueeze(dim=0), cropCenter[i])\n",
    "        self.gaussian = self.step7(self.layer6)\n",
    "        \n",
    "        return self.cropFeature, self.gaussian[:,0]\n",
    "        \n",
    "class LocalEncoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LocalEncoder, self).__init__()\n",
    "        self.input = 0\n",
    "        self.layer1 = self.layer2 = self.layer3 = self.layer4 = 0\n",
    "        self.output = 0\n",
    "\n",
    "        self.step1 = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 1), torch.nn.Dropout(),\n",
    "            torch.nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 3, padding = 1),\n",
    "            torch.nn.BatchNorm2d(512), torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1),\n",
    "            torch.nn.BatchNorm2d(512), torch.nn.ReLU())\n",
    "        \n",
    "        self.step2 = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(kernel_size = 2, stride = 2), torch.nn.Dropout(),\n",
    "            torch.nn.Conv2d(in_channels = 512, out_channels = 1024, kernel_size = 3, padding = 1),\n",
    "            torch.nn.BatchNorm2d(1024), torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels = 1024, out_channels = 256, kernel_size = 3, padding = 1),\n",
    "            torch.nn.BatchNorm2d(256), torch.nn.ReLU())\n",
    "        \n",
    "        self.step3 = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(start_dim=1),\n",
    "            torch.nn.Linear(256*4*4, 256),\n",
    "            torch.nn.BatchNorm1d(256),\n",
    "            torch.nn.ReLU())\n",
    "\n",
    "        self.step4 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.BatchNorm1d(128),\n",
    "            torch.nn.ReLU())\n",
    "\n",
    "        self.step5 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 2),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Softmax(dim=1))\n",
    "        \n",
    "    def forward(self, cropImages): \n",
    "        self.input = cropImages\n",
    "        self.layer1 = self.step1(self.input)\n",
    "        self.layer2 = self.step2(self.layer1)\n",
    "        self.layer3 = self.step3(self.layer2)\n",
    "        self.layer4 = self.step4(self.layer3)\n",
    "        self.output = self.step5(self.layer4)\n",
    "\n",
    "        return self.output[:,0]\n",
    "        \n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.input = 0\n",
    "        self.globalInputFeature = self.localInputFeature = 0\n",
    "        self.globalDynamicFeature = self.localDynamicFeature = 0\n",
    "        self.cropFeature = 0\n",
    "        # output\n",
    "        self.gaussian = self.output = 0\n",
    "        \n",
    "        # input encoder\n",
    "        self.globalInputEncoder = [InputEncoder().to(device) for i in range(3)]\n",
    "        self.localInputEncoder = [InputEncoder().to(device) for i in range(3)]\n",
    "        self.globalDynamicEncoder = DynamicEncoder().to(device)\n",
    "        self.localDynamicEncoder = DynamicEncoder().to(device)\n",
    "        \n",
    "        # encoder\n",
    "        self.globalEncoder = GlobalEncoder().to(device)\n",
    "        self.localEncoder = LocalEncoder().to(device)\n",
    "        \n",
    "    def forward(self, fullImages, cropImages, cropCenter):\n",
    "        # extract center slice\n",
    "        fullImages = fullImages[:,:,1]\n",
    "        cropImages = cropImages[:,:,1]\n",
    "    \n",
    "        # encode input with each encoder\n",
    "        self.globalInputFeature = torch.zeros(len(fullImages), 32*4, IMAGE_SIZE, IMAGE_SIZE)\n",
    "        self.localInputFeature = torch.zeros(len(cropImages), 32*4, WINDOW_SIZE, WINDOW_SIZE)\n",
    "        \n",
    "        # T2WI Input\n",
    "        self.globalInputFeature[:,0:32] = self.globalInputEncoder[0](torch.unsqueeze(fullImages[:,0], dim=1))\n",
    "        self.localInputFeature[:,0:32] = self.localInputEncoder[0](torch.unsqueeze(cropImages[:,0], dim=1))\n",
    "        # ADC Input\n",
    "        self.globalInputFeature[:,32:64] = self.globalInputEncoder[1](torch.unsqueeze(fullImages[:,1], dim=1))\n",
    "        self.localInputFeature[:,32:64] = self.localInputEncoder[1](torch.unsqueeze(cropImages[:,1], dim=1))\n",
    "        # CE-T1WI Input\n",
    "        self.globalInputFeature[:,64:96] = self.globalInputEncoder[2](torch.unsqueeze(fullImages[:,-1], dim=1))\n",
    "        self.localInputFeature[:,64:96] = self.localInputEncoder[2](torch.unsqueeze(cropImages[:,-1], dim=1))\n",
    "        # Dynamic T1WI Input\n",
    "        self.globalInputFeature[:,96:128] = self.globalDynamicEncoder(torch.unsqueeze(fullImages[:,2:22], dim=1))\n",
    "        self.localInputFeature[:,96:128] = self.localDynamicEncoder(torch.unsqueeze(cropImages[:,2:22], dim=1))\n",
    "\n",
    "        # global encode\n",
    "        self.cropFeature, self.gaussian = self.globalEncoder(self.globalInputFeature.to(device), cropCenter)\n",
    "        # local encode\n",
    "        self.output = self.localEncoder(torch.cat([self.localInputFeature, self.cropFeature], dim = 1).to(device))\n",
    "\n",
    "        return self.gaussian, self.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e65bac",
   "metadata": {},
   "source": [
    "### Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55ab29ed",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def saveOverlay(filename, pil_img, pil_map, color='jet', vmin=VMIN, vmax=VMAX):\n",
    "    img = np.asarray(pil_img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    map_img = np.asarray(pil_map)\n",
    "    \n",
    "    lim_img = np.where(map_img < vmin, 0, map_img)\n",
    "    lim_img = ((lim_img - vmin) / (vmax-vmin) * 255).astype('uint8')\n",
    "    \n",
    "    heatmap = cv2.applyColorMap(lim_img, cv2.COLORMAP_JET)\n",
    "    map_img = cv2.cvtColor(map_img, cv2.COLOR_GRAY2RGB)\n",
    "    heatmap = np.where(map_img < vmin, 0, heatmap)\n",
    "    \n",
    "    superimposed_img = heatmap * 0.4 + img\n",
    "    cv2.imwrite(filename, superimposed_img)\n",
    "    \n",
    "def saveImage(filename, pil_image):\n",
    "    img = np.asarray(pil_image)\n",
    "    cv2.imwrite(filename, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fd6a7f",
   "metadata": {},
   "source": [
    "### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c70dd3a8",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_1_1\n",
      "1_2_1\n",
      "2_1_1\n",
      "2_2_1\n",
      "2_3_1\n",
      "3_1_1\n",
      "4_1_1\n",
      "4_2_1\n",
      "5_1_1\n",
      "5_2_1\n",
      "6_1_1\n",
      "6_2_1\n",
      "7_1_1\n",
      "7_2_1\n",
      "8_1_1\n",
      "9_1_1\n",
      "10_1_1\n",
      "11_1_1\n",
      "11_2_1\n",
      "12_1_1\n",
      "13_1_1\n",
      "13_2_1\n",
      "14_1_1\n",
      "14_2_1\n",
      "14_3_1\n",
      "15_1_1\n",
      "15_2_1\n",
      "16_1_1\n",
      "16_2_1\n",
      "17_1_1\n",
      "17_2_1\n",
      "17_3_1\n",
      "18_1_1\n",
      "18_2_1\n",
      "19_1_1\n",
      "20_1_1\n",
      "21_1_1\n",
      "21_2_1\n",
      "22_1_1\n",
      "22_2_1\n",
      "23_1_1\n",
      "23_2_1\n",
      "24_1_1\n",
      "25_1_1\n",
      "25_2_1\n",
      "26_1_1\n",
      "26_2_1\n",
      "27_1_1\n",
      "27_2_1\n",
      "28_1_1\n",
      "28_2_1\n",
      "29_1_1\n",
      "29_2_1\n",
      "31_1_1\n",
      "31_2_1\n",
      "32_1_1\n",
      "33_1_1\n",
      "33_2_1\n",
      "34_1_1\n",
      "34_2_1\n",
      "35_1_1\n",
      "36_1_1\n",
      "36_2_1\n",
      "37_1_1\n",
      "37_2_1\n",
      "39_1_1\n",
      "39_2_1\n",
      "40_1_1\n",
      "41_1_1\n",
      "42_1_1\n",
      "43_1_1\n",
      "43_2_1\n",
      "44_1_1\n",
      "44_2_1\n",
      "45_1_1\n",
      "46_1_1\n",
      "47_1_1\n",
      "47_2_1\n",
      "48_1_1\n",
      "49_1_1\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "valid_dataset_fullImage = Dataset_FullImage(valid_files, valid_labels)\n",
    "valid_dataset_cropImage = Dataset_CropImage(valid_files, valid_labels)\n",
    "\n",
    "# for faster\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# model\n",
    "net = Net().to(device)\n",
    "net.load_state_dict(torch.load(LOAD_DIR + LOAD_FILE))\n",
    "net.train(False)\n",
    "\n",
    "for data_num in range(int(len(valid_dataset_fullImage)/20)):\n",
    "    with torch.no_grad():\n",
    "        fullImage_info, fullImages, gaussianRoi = valid_dataset_fullImage[data_num*20]\n",
    "        cropImage_info, cropImages = valid_dataset_cropImage[data_num*20]\n",
    "        fullImages, cropImages = fullImages.to(device), cropImages.to(device)\n",
    "\n",
    "        print(fullImage_info)\n",
    "\n",
    "        output = torch.zeros(IMAGE_SIZE+WINDOW_SIZE-1, IMAGE_SIZE+WINDOW_SIZE-1)\n",
    "        for i in range(len(cropImages)):\n",
    "            for j in range(len(cropImages[0])):\n",
    "                cropCenter = np.array([i*WINDOW_STRIDE, j*WINDOW_STRIDE], int)\n",
    "                gaussian, prob = net(torch.unsqueeze(fullImages, dim=0), torch.unsqueeze(cropImages[i,j], dim=0), np.expand_dims(cropCenter,axis=0))\n",
    "                cropCenter += int(WINDOW_SIZE/2)\n",
    "                gaussian_prob = torch.zeros(WINDOW_SIZE, WINDOW_SIZE)\n",
    "                # gaussian_prob[int(WINDOW_SIZE/2),int(WINDOW_SIZE/2)] = prob\n",
    "                for k in range(WINDOW_SIZE):\n",
    "                    for l in range(WINDOW_SIZE):\n",
    "                        gaussian_prob[k,l] = prob * np.exp( -((int(WINDOW_SIZE/2)-k)**2 + (int(WINDOW_SIZE/2)-l)**2) / (2*((WINDOW_SIGMA+1)**2)) )\n",
    "                \n",
    "                output[cropCenter[0]-int(WINDOW_SIZE/2):cropCenter[0]+int(WINDOW_SIZE/2)+1, cropCenter[1]-int(WINDOW_SIZE/2):cropCenter[1]+int(WINDOW_SIZE/2)+1] += gaussian_prob\n",
    "\n",
    "        output = output[int(WINDOW_SIZE/2):int(WINDOW_SIZE/2)+IMAGE_SIZE, int(WINDOW_SIZE/2):int(WINDOW_SIZE/2)+IMAGE_SIZE]\n",
    "        output /= torch.max(output)\n",
    "        pil_image = torchvision.transforms.functional.to_pil_image(output)\n",
    "        pil_map = torchvision.transforms.functional.to_pil_image(fullImages[21,1])\n",
    "        saveOverlay(SAVE_DIR + fullImage_info + '_pred.png', pil_map, pil_image)\n",
    "\n",
    "        pil_map = torchvision.transforms.functional.to_pil_image(gaussianRoi)\n",
    "        saveOverlay(SAVE_DIR + fullImage_info + '_ans.png', pil_map, pil_image)\n",
    "\n",
    "        pil_image = torchvision.transforms.functional.to_pil_image(fullImages[0,1])\n",
    "        saveImage(SAVE_DIR + fullImage_info + '_t2.png', pil_image)\n",
    "        pil_image = torchvision.transforms.functional.to_pil_image(fullImages[1,1])\n",
    "        saveImage(SAVE_DIR + fullImage_info + '_adc.png', pil_image)\n",
    "        pil_image = torchvision.transforms.functional.to_pil_image(fullImages[2,1])\n",
    "        saveImage(SAVE_DIR + fullImage_info + '_t1.png', pil_image)\n",
    "        pil_image = torchvision.transforms.functional.to_pil_image(fullImages[21,1])\n",
    "        saveImage(SAVE_DIR + fullImage_info + '_cet1.png', pil_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
